---
title: DAG-GNN 阅读摘录
date: 2024-11-02 18:10:00 +0800
categories: [causal inference]
tags: [Machine Learning, causal inference, causal discovery]     
math: true
---

## DAG-GNN

### Abstract

**领域**：因果推断中的因果发现（学习可信的DAG）
**Motivation**: DAG的搜索空间随着节点数增长超指数增加；旧有的方法（平滑优化方法）有限制（不能捕捉非线性关系）；深度学习具有捕捉复杂非线性映射关系的能力
**工作**：提出一个深度生成模型并且应用结构约束的一个变种去学习DAG（其中的自编码器部分是核心（DAG-GNN））
**优势**：能够将离散变量处理成等价向量；在模拟数据中学习非线性生成样本的效果更好；在benchmark数据集中学习出的DAG和全局最优结果非常接近

### Main Part: Neural DAG Structure Learning

#### 线性SEM

Definition of SEM model:
![[Pasted image 20241020092600.png]]
其中，X是m个变量的联合分布，A是$m\times m$的邻接矩阵，Z是与X相同大小的噪声；
如果DAG的各个节点按照拓扑顺序排序，那么邻接矩阵A就是严格的上三角矩阵；
那么假如我们知道了DAG，我们对X进行祖先采样（每个节点只有它的父节点都完成采样之后它才能被采样）相当于生成一个随机噪声Z然后计算下面的公式：
![[Pasted image 20241020093801.png]]

#### GNN 模型

**标准GCN(图卷积神经网络)**
![[Pasted image 20241020094510.png]]
$\widehat{A}$ 为A的标准化矩阵，$W^1$，$W^2$分别为参数矩阵

基于该标准GCN做的改动：![[Pasted image 20241020094907.png]]
其中f1，f2是参数化函数（很有可能是非线性的），如果f2可逆，则有![[Pasted image 20241020095136.png]]

#### 用VAE进行模型学习

通过最大化**evidence lower bound(ELBO)** 来用近似后验分布q(Z|X)逼近真实后验分布p(Z|X)
![[Pasted image 20241020095711.png]]![[Pasted image 20241020095717.png]]
解释：
通过计算KL散度来计算p和q之间的距离；右式后一项称为证据（因为KL散度永远大于0，故证据永远大于证据下界，当且仅当q分布与p分布完全相等时证据与证据下界完全相等）

VAE: Encoder: ![[Pasted image 20241020101717.png]]
Decoder: ![[Pasted image 20241020101737.png]]
之前已经推导出关于生成X（decoder）的公式，现在建立Encoder的公式![[Pasted image 20241020101908.png]]

#### 模型结构

![[Pasted image 20241020102218.png]]
对于encoder，f3是多层感知机，f4是恒等映射（等于自身）：
![[Pasted image 20241020102649.png]]
设定MLP为![[Pasted image 20241020102830.png]]

对于decoder，f1是恒等映射，f2是MLP：
![[Pasted image 20241020102950.png]]

可能想要在公式中交换MLP和恒等映射，但是发现效果下降，推测可能是现有的设计强调了对于线性SEM的非线性扩展

计算KL散度：![[Pasted image 20241020103559.png]]
计算证据：![[Pasted image 20241020103644.png]]
其中，c是常量，![[Pasted image 20241020103740.png]]被认为是在服从近似后验分布q的噪声Z的蒙特卡罗采样

#### 离散变量

针对变量的离散特性，我们需要对decoder做调整（将f2由恒等映射替换为softmax）
![[Pasted image 20241020105125.png]]
与之对应，证据公式也需要修改：![[Pasted image 20241020105529.png]]

#### 特例：线性SEM

损失函数简化为：
![[Pasted image 20241020110354.png]]
其中：![[Pasted image 20241020110407.png]]
结果与DAG with NOTEARS论文中损失函数相同

#### 有向无环性

对无环的约束做了调整，将矩阵指数替换为矩阵的幂
![[截屏2024-11-02 19.34.13.png]]

### 实验

#### 线性情况

![[截屏2024-11-02 19.42.06.png]]
依然和NOTEARS表现相当

#### 非线性情况

对输入进行非线性处理，DAG-GNN优于NOTEARS
![[截屏2024-11-02 19.48.26.png]]
对输入和邻接矩阵都进行非线性处理，DAG-GNN优势更加显著
![[截屏2024-11-02 19.49.10.png]]

#### 输入的数值为向量

![[截屏2024-11-02 19.50.21.png]]

#### 和groundtruth算法进行比较

尽管DAG-GNN算法无法收敛到全局最优点（与groundtruth存在差距的原因可能是因为encoder结构过于简单），但是也能够处理离散变量并收敛到一个稳定点
![[截屏2024-11-02 19.51.10.png]]

#### 在真实数据上的表现

![[截屏2024-11-02 19.55.54.png]]
