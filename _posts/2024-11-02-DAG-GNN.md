---
title: DAG-GNN 阅读摘录
date: 2024-11-02 18:10:00 +0800
categories: [causal inference]
tags: [Machine Learning, causal inference, causal discovery]     
math: true
---

## DAG-GNN

### Abstract

**领域**：因果推断中的因果发现（学习可信的DAG）
**Motivation**: DAG的搜索空间随着节点数增长超指数增加；旧有的方法（平滑优化方法）有限制（不能捕捉非线性关系）；深度学习具有捕捉复杂非线性映射关系的能力
**工作**：提出一个深度生成模型并且应用结构约束的一个变种去学习DAG（其中的自编码器部分是核心（DAG-GNN））
**优势**：能够将离散变量处理成等价向量；在模拟数据中学习非线性生成样本的效果更好；在benchmark数据集中学习出的DAG和全局最优结果非常接近

### Main Part: Neural DAG Structure Learning

#### 线性SEM

Definition of SEM model:
![Pasted image 20241020092600](https://s2.loli.net/2024/11/02/fqikuJ8v26bgCdK.png)
其中，X是m个变量的联合分布，A是$m\times m$的邻接矩阵，Z是与X相同大小的噪声；
如果DAG的各个节点按照拓扑顺序排序，那么邻接矩阵A就是严格的上三角矩阵；
那么假如我们知道了DAG，我们对X进行祖先采样（每个节点只有它的父节点都完成采样之后它才能被采样）相当于生成一个随机噪声Z然后计算下面的公式：
![Pasted image 20241020093801](https://s2.loli.net/2024/11/02/dkwJinWhg68FAHx.png)

#### GNN 模型

**标准GCN(图卷积神经网络)**
![Pasted image 20241020094510](https://s2.loli.net/2024/11/02/XMbVc7C2ZQLUtWl.png)
$\widehat{A}$ 为A的标准化矩阵，$W^1$，$W^2$分别为参数矩阵

基于该标准GCN做的改动：![Pasted image 20241020094907](https://s2.loli.net/2024/11/02/es91rDM8Rnf25Sw.png)
其中f1，f2是参数化函数（很有可能是非线性的），如果f2可逆，则有![Pasted image 20241020095136](https://s2.loli.net/2024/11/02/rpVFv7YxE4eShfj.png)

#### 用VAE进行模型学习

通过最大化**evidence lower bound(ELBO)** 来用近似后验分布q(Z|X)逼近真实后验分布p(Z|X)
![Pasted image 20241020095711](https://s2.loli.net/2024/11/02/Tt6foVPRDSBaCAd.png)![Pasted image 20241020095717](https://s2.loli.net/2024/11/02/BMVa6QqEeTPzRCG.png)
解释：
通过计算KL散度来计算p和q之间的距离；右式后一项称为证据（因为KL散度永远大于0，故证据永远大于证据下界，当且仅当q分布与p分布完全相等时证据与证据下界完全相等）

VAE: Encoder: ![Pasted image 20241020101717](https://s2.loli.net/2024/11/02/tFjYbGZoQ2S3KDw.png)
Decoder: ![Pasted image 20241020101737](https://s2.loli.net/2024/11/02/TDf3pH2dMbZ4loe.png)
之前已经推导出关于生成X（decoder）的公式，现在建立Encoder的公式![Pasted image 20241020101908](https://s2.loli.net/2024/11/02/fe8ZPK3nUw4Blzh.png)

#### 模型结构

![Pasted image 20241020102218](https://s2.loli.net/2024/11/02/DVQI51i6eOJHCTb.png)
对于encoder，f3是多层感知机，f4是恒等映射（等于自身）：
![Pasted image 20241020102649](https://s2.loli.net/2024/11/02/djo9aLT1WKMDmbU.png)
设定MLP为![Pasted image 20241020102830](https://s2.loli.net/2024/11/02/rApxwEaIPv1J3KM.png)

对于decoder，f1是恒等映射，f2是MLP：
![Pasted image 20241020102950](https://s2.loli.net/2024/11/02/rbh2Z7zgcTBANYK.png)

可能想要在公式中交换MLP和恒等映射，但是发现效果下降，推测可能是现有的设计强调了对于线性SEM的非线性扩展

计算KL散度：![Pasted image 20241020103559](https://s2.loli.net/2024/11/02/gX1YmaWonVjRHhb.png)
计算证据：![Pasted image 20241020103644](https://s2.loli.net/2024/11/02/fM9lz3jQwBnckDa.png)
其中，c是常量，![Pasted image 20241020103740](https://s2.loli.net/2024/11/02/D2oEbOeq61xP479.png)被认为是在服从近似后验分布q的噪声Z的蒙特卡罗采样

#### 离散变量

针对变量的离散特性，我们需要对decoder做调整（将f2由恒等映射替换为softmax）
![Pasted image 20241020105125](https://s2.loli.net/2024/11/02/Frf9VeowMG1EHiO.png)
与之对应，证据公式也需要修改：![Pasted image 20241020105529](https://s2.loli.net/2024/11/02/aFkHsStENGhwMjy.png)

#### 特例：线性SEM

损失函数简化为：
![Pasted image 20241020110354](https://s2.loli.net/2024/11/02/XqsB28MATYdVlfI.png)
其中：![Pasted image 20241020110407](https://s2.loli.net/2024/11/02/Ljyflu8dhismz1b.png)
结果与DAG with NOTEARS论文中损失函数相同

#### 有向无环性

对无环的约束做了调整，将矩阵指数替换为矩阵的幂
![截屏2024-11-02 19.34.13](https://s2.loli.net/2024/11/02/roG9eXc5px2YfUS.png)

### 实验

#### 线性情况

![截屏2024-11-02 19.42.06](https://s2.loli.net/2024/11/02/htO9Ejgv3IKydVQ.png)
依然和NOTEARS表现相当

#### 非线性情况

对输入进行非线性处理，DAG-GNN优于NOTEARS
![截屏2024-11-02 19.48.26](https://s2.loli.net/2024/11/02/4gGyblH7sAjTu2X.png)
对输入和邻接矩阵都进行非线性处理，DAG-GNN优势更加显著
![截屏2024-11-02 19.49.10](https://s2.loli.net/2024/11/02/NSyLUYkQHA9b4OC.png)

#### 输入的数值为向量

![截屏2024-11-02 19.50.21](https://s2.loli.net/2024/11/02/WZmI9EA1DYQg65P.png)

#### 和groundtruth算法进行比较

尽管DAG-GNN算法无法收敛到全局最优点（与groundtruth存在差距的原因可能是因为encoder结构过于简单），但是也能够处理离散变量并收敛到一个稳定点
![截屏2024-11-02 19.51.10](https://s2.loli.net/2024/11/02/SM9YIp4GxPzDNAX.png)

#### 在真实数据上的表现

![截屏2024-11-02 19.55.54](https://s2.loli.net/2024/11/02/QD5ifChJbrnEj8l.png)
